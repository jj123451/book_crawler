{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52bee98d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't find program: 'false'\n"
     ]
    }
   ],
   "source": [
    "%%script false --no-raise-error\n",
    "import json\n",
    "from google.colab import userdata\n",
    "from google.oauth2 import service_account\n",
    "from google.cloud.bigquery import magics\n",
    "\n",
    "credentials_json = userdata.get('BIGQUERY_CREDENTIALS')\n",
    "credentials = service_account.Credentials.from_service_account_info(json.loads(credentials_json))\n",
    "magics.context.credentials = credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a93d18b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The bigquery_magics extension is already loaded. To reload it, use:\n",
      "  %reload_ext bigquery_magics\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import bigquery\n",
    "from google.cloud.bigquery import magics\n",
    "%load_ext bigquery_magics\n",
    "\n",
    "data_set = \"testing_set\"\n",
    "project_name = \"emerald-entity-468916-f9\"\n",
    "endpoint = \"gemini-2.5-flash-lite\"\n",
    "connection_id = \"us.ai_connection\"\n",
    "\n",
    "job_config = bigquery.QueryJobConfig(default_dataset = f\"{project_name}.{data_set}\")\n",
    "client = bigquery.Client(project = project_name, default_query_job_config = job_config, credentials = globals().get('credentials', None))\n",
    "magics.context.default_query_job_config = job_config\n",
    "magics.context.project = project_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49956407",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edc3c21705e54961b412ef9fc37f0f6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Query is running:   0%|          |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%bigquery --pyformat\n",
    "\n",
    "CREATE OR REPLACE MODEL `gemini-embedding-001`\n",
    "REMOTE WITH CONNECTION `{connection_id}`\n",
    "OPTIONS(ENDPOINT = 'gemini-embedding-001');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d80ac3e",
   "metadata": {},
   "source": [
    "### Phase 2 - Correcting Books\n",
    "**Book correction performs correction of OCR book scanning errors. Such errors are quite common in books from GDELT Processes Internet Archive.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6b41a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93d3811659664845a6f5c7c1514f5fdc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Query is running:   0%|          |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%bigquery --pyformat\n",
    "CREATE OR REPLACE PROCEDURE phase2_correction_correct()\n",
    "BEGIN\n",
    "    DECLARE correct_model_params JSON DEFAULT JSON '''\n",
    "    {{\"systemInstruction\": {{\"parts\": [{{\"text\": \"You are an assistant helping to fix the content of scanned books.\"}}]}}}}\n",
    "    ''';\n",
    "\n",
    "    -- corrects OCR error in given book fragment\n",
    "    UPDATE {data_set}.tmp_correction_chunks b SET b.corrected_txt = \n",
    "        AI.GENERATE(FORMAT(p.prompt, b.prefix, b.original_txt, b.suffix), \n",
    "        connection_id => '{connection_id}', endpoint => '{endpoint}',\n",
    "        model_params => correct_model_params).result\n",
    "    FROM {data_set}.prompts p WHERE b.corrected_txt IS NULL AND p.code = 'correct';\n",
    "\n",
    "    -- concatenates corrected fragments and saves it in 'book' table\n",
    "    MERGE {data_set}.books b\n",
    "    USING (SELECT book_id, STRING_AGG(corrected_txt, ' ' ORDER BY chunk_number) AS aggregated_txt\n",
    "    FROM {data_set}.tmp_correction_chunks GROUP BY book_id) cb\n",
    "    ON b.book_id = cb.book_id AND b.corrected_txt IS NULL\n",
    "    WHEN MATCHED THEN UPDATE SET corrected_txt = cb.aggregated_txt;\n",
    "END;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aaa3ffe",
   "metadata": {},
   "source": [
    "### Phase 3 - Chunking books\n",
    "**Simple divide of each book to overlaping fragments.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9a4aac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74f966c5db9c493680b9bd59b5791eef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Query is running:   0%|          |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%bigquery --pyformat\n",
    "CREATE OR REPLACE PROCEDURE phase3_prepare_chunks()\n",
    "BEGIN\n",
    "    -- divides book to 40000 long fragments\n",
    "    CREATE OR REPLACE TABLE {data_set}.tmp_overlapped_chunks AS\n",
    "    SELECT book_id, chunk_num - 1 as chunk_number, SUBSTR(corrected_txt, (chunk_num - 1) * 40000 + 1, 40000) as txt, CAST(null AS STRING) text_with_overlap\n",
    "    FROM {data_set}.books, UNNEST(GENERATE_ARRAY(1, CAST(CEIL(LENGTH(corrected_txt) / 40000) AS INT64))) as chunk_num\n",
    "    WHERE processed = False or processed is null;\n",
    "\n",
    "    -- adds predecessing overlap to each fragment\n",
    "    UPDATE {data_set}.tmp_overlapped_chunks curr set curr.text_with_overlap = curr.txt || ' ' || RIGHT(prev.txt, 4000)\n",
    "    FROM {data_set}.tmp_overlapped_chunks prev WHERE curr.book_id = prev.book_id AND curr.chunk_number = prev.chunk_number + 1;\n",
    "    -- the very first fragment has no overlap\n",
    "    UPDATE {data_set}.tmp_overlapped_chunks set text_with_overlap = txt where text_with_overlap is null;\n",
    "\n",
    "    -- stores fragments in 'chunks' table\n",
    "    CREATE OR REPLACE TABLE {data_set}.chunks(book_id STRING, chunk_number INTEGER, txt STRING, summary STRING,\n",
    "        fragment_number STRING, characters_id_data STRING, characters_full_data STRING);\n",
    "\n",
    "    INSERT INTO {data_set}.chunks (book_id, chunk_number, txt)\n",
    "    (SELECT book_id, chunk_number, text_with_overlap FROM {data_set}.tmp_overlapped_chunks\n",
    "    EXCEPT DISTINCT SELECT book_id, chunk_number, txt FROM {data_set}.chunks);\n",
    "END;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a3313c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4df33a4a41b243afbe6aac0c50bd42c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Query is running:   0%|          |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%bigquery --pyformat\n",
    "\n",
    "CREATE OR REPLACE PROCEDURE phase4_fragments_summarization()\n",
    "BEGIN\n",
    "\t-- summarize the fragment to given size, calculated in such way that all fragments summaries together have no more then 80000 chars\n",
    "  \tUPDATE {data_set}.chunks c SET summary = \n",
    "\t\tCOALESCE(AI.GENERATE(FORMAT(p.prompt, s.size, c.txt), \n",
    "\t\tconnection_id => '{connection_id}', endpoint => '{endpoint}').result, '')\n",
    "    FROM (select book_id, cast(80000 / count(*) as INT64) size from {data_set}.chunks group by book_id) s, {data_set}.prompts p\n",
    "    WHERE c.book_id = s.book_id AND c.summary IS NULL AND p.code = 'summarize';\n",
    "END;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc55db6",
   "metadata": {},
   "source": [
    "### Phase 4 - Summarizing\n",
    "**Prepare concise summary of each book. It will be supplementary information in several inference operations later.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30345cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41cf4cb023024a25a39a8de596a521bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Query is running:   0%|          |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%bigquery --pyformat\n",
    "CREATE OR REPLACE PROCEDURE phase4_summarize_fragments_summaries()\n",
    "BEGIN\n",
    "\t-- summarize the whole book based on fragments summaries, by the way ensuring that the final summary size is reduced to 20000 chars\n",
    "\tUPDATE {data_set}.books b SET b.summary = COALESCE(AI.GENERATE(FORMAT(p.prompt, s.summary), \n",
    "\t\t\tconnection_id => '{connection_id}', endpoint => '{endpoint}').result, '')\n",
    "\tFROM {data_set}.prompts p,\n",
    "        (SELECT book_id, STRING_AGG(summary, '/n' ORDER BY chunk_number) summary FROM {data_set}.chunks GROUP BY book_id) s \n",
    "    WHERE p.code = 'reduce_summary' AND b.book_id = s.book_id AND b.summary is null;\n",
    "END;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876b71d8",
   "metadata": {},
   "source": [
    "### Phase 5 - Character identification\n",
    "**The most crucial part of whole project identifying consistent characters across the whole book**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05ecb35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2749357483004f7ba1f7db167fce1f4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Query is running:   0%|          |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%bigquery --pyformat\n",
    "CREATE OR REPLACE PROCEDURE phase5_prepare_character_ids_from_fragments()\n",
    "BEGIN\n",
    "    -- sets string fragment number, used later to identify fragments where given character appears\n",
    "    UPDATE {data_set}.chunks SET fragment_number = FORMAT('%03d', chunk_number + 1) where fragment_number is null;\n",
    "\n",
    "    -- extract full names and other identification-helping information from book fragments\n",
    "    UPDATE {data_set}.chunks c SET c.characters_id_data = \n",
    "        AI.GENERATE(FORMAT(p.prompt, c.txt), connection_id => '{connection_id}', endpoint => '{endpoint}').result\n",
    "    FROM {data_set}.prompts p, {data_set}.books b\n",
    "    WHERE p.code = 'characters_id_data' and c.book_id = b.book_id;\n",
    "\n",
    "    -- split result: each character in separate row\n",
    "    CREATE OR REPLACE TABLE {data_set}.tmp_characters_id_data AS\n",
    "    (select book_id, chunk_number, fragment_number, x as characters_id_data,\n",
    "    CAST(NULL AS STRING) full_name, CAST(NULL AS STRING) information, CAST(NULL AS INT64) importance\n",
    "    FROM {data_set}.chunks, UNNEST(JSON_QUERY_ARRAY(TRIM(characters_id_data, '`json\\n'), '$')) x);\n",
    "\n",
    "    -- add id (unique together with book id)\n",
    "    CREATE OR REPLACE TABLE {data_set}.identifiers AS\n",
    "    SELECT row_number() OVER(ORDER BY book_id, chunk_number, full_name) id, * from {data_set}.tmp_characters_id_data;\n",
    "\n",
    "    -- extract fields from json\n",
    "    UPDATE {data_set}.identifiers SET full_name = json_value(characters_id_data, '$.full_name'), information = json_value(characters_id_data, '$.information'),\n",
    "        importance = CAST(JSON_VALUE(characters_id_data, '$.importance') AS INT64) where full_name is null;\n",
    "\n",
    "    -- correct potential errors in 'importance' count\n",
    "    UPDATE {data_set}.identifiers SET importance = 1 where importance = 0 or importance is null;\n",
    "END;\n",
    "-- author: jj123451"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133a6b38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08eccec0127c4fb98cb8e7bd3960537c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Query is running:   0%|          |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%bigquery --pyformat\n",
    "CREATE OR REPLACE PROCEDURE phase5_prepare_character_ids_initial_embeddings()\n",
    "BEGIN\n",
    "  -- create embeddings for all characters identifying information (with full names)\n",
    "  CREATE OR REPLACE TABLE {data_set}.tmp_characters_id_embeddings as SELECT * FROM ML.GENERATE_EMBEDDING(\n",
    "    MODEL `{data_set}.gemini-embedding-001`,\n",
    "    (SELECT characters_id_data as content, full_name as title, id, book_id, chunk_number, FALSE is_query FROM {data_set}.identifiers),\n",
    "    STRUCT(TRUE AS flatten_json_output, 'RETRIEVAL_DOCUMENT' as task_type));\n",
    "END;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2eaf0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca482473db1d4af9bdaeb89134cd2cfc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Query is running:   0%|          |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%bigquery --pyformat\n",
    "CREATE OR REPLACE PROCEDURE phase5_merge_characters_duplicates_with_return_param(OUT updated INT64)  \n",
    "BEGIN\n",
    "  -- search characters most similar to each other\n",
    "\tCREATE OR REPLACE TABLE {data_set}.tmp_characters_id_distance as\n",
    "  SELECT query.id query_id, query.title query_full_name, base.id, base.title full_name, base.chunk_number, base.book_id, query.book_id query_book_id, distance FROM\n",
    "  VECTOR_SEARCH(\n",
    "\t(SELECT id, book_id, chunk_number, title, ml_generate_embedding_result FROM {data_set}.tmp_characters_id_embeddings),\n",
    "\t'ml_generate_embedding_result',\n",
    "\t(SELECT id, title, book_id, ml_generate_embedding_result FROM {data_set}.tmp_characters_id_embeddings),\n",
    "\t'ml_generate_embedding_result',\n",
    "\ttop_k => 2);\n",
    "  -- !TODO! - achange above query to search within data of the same book. It very rarely match characters from different books, but still\n",
    "  DELETE FROM {data_set}.tmp_characters_id_distance where book_id <> query_book_id;\n",
    "\n",
    "  -- delete distances to self and duplicate distances\n",
    "  DELETE FROM {data_set}.tmp_characters_id_distance where query_id = id;\n",
    "  DELETE FROM {data_set}.tmp_characters_id_distance d where d.id < d.query_id and exists (select 1 from {data_set}.tmp_characters_id_distance dd where dd.id = d.query_id and d.id = dd.query_id);\n",
    "\n",
    "  -- prepara pairs of duplicate candidates, each pair in group belongs to the same fragment and group is no longer then 50 characters\n",
    "  CREATE OR REPLACE TABLE {data_set}.tmp_characters_duplicate_candidates as\n",
    "  select string_agg(pair, ',\\n') pairs, book_id, chunk_number from\n",
    "  (select TO_JSON_STRING(STRUCT(query.id, query.full_name as first_individual_full_name, query.information as first_individual_information, \n",
    "  base.full_name as second_individual_full_name, base.information as second_individual_information), true) pair, embed.book_id, query.chunk_number, query.fragment_number,\n",
    "  CEIL(ROW_NUMBER() OVER (PARTITION BY embed.book_id, query.chunk_number ORDER BY query.id) / 50.0) chunk_50_nr\n",
    "  from {data_set}.tmp_characters_id_distance embed\n",
    "  JOIN {data_set}.identifiers query ON embed.query_id = query.id\n",
    "  JOIN {data_set}.identifiers base ON embed.id = base.id)\n",
    "  group by book_id, chunk_number, chunk_50_nr;\n",
    "\n",
    "  -- use AI to check the duplicate candidates and assess which ones are in fact the same person\n",
    "  CREATE OR REPLACE TABLE {data_set}.tmp_the_same_characters as SELECT pairs.book_id, pairs.chunk_number,\n",
    "  AI.GENERATE(FORMAT(p.prompt, pairs.pairs, cb.summary), connection_id => '{connection_id}', endpoint => '{endpoint}').result pairs\n",
    "  FROM {data_set}.prompts p, {data_set}.tmp_characters_duplicate_candidates pairs, {data_set}.books cb\n",
    "  WHERE p.code = 'find_the_same_characters' and pairs.book_id = cb.book_id;\n",
    "  -- check AI results from above (AI sometimes makes mistakes, rarely, but consequence is mergin two different characters together!)\n",
    "  CREATE OR REPLACE TABLE {data_set}.tmp_different_characters as SELECT pairs.book_id, pairs.chunk_number,\n",
    "  AI.GENERATE(FORMAT(p.prompt, pairs.pairs, cb.summary), connection_id => '{connection_id}', endpoint => '{endpoint}').result pairs\n",
    "  FROM {data_set}.prompts p, {data_set}.tmp_the_same_characters pairs, {data_set}.books cb\n",
    "  WHERE p.code = 'find_different_characters' and pairs.book_id = cb.book_id;\n",
    "\n",
    "  -- split both results of AI inference\n",
    "  CREATE OR REPLACE TABLE {data_set}.tmp_the_same_characters_split AS\n",
    "  select book_id, chunk_number, pair, CAST(NULL AS INT64) id\n",
    "  from {data_set}.tmp_the_same_characters, UNNEST(JSON_QUERY_ARRAY(TRIM(pairs, '`json\\n'), '$')) pair;\t\t\n",
    "  UPDATE {data_set}.tmp_the_same_characters_split SET id = CAST(JSON_VALUE(pair, '$.id') AS INT64) where id is null;\n",
    "\n",
    "  CREATE OR REPLACE TABLE {data_set}.tmp_different_characters_split AS\n",
    "  select book_id, chunk_number, pair, CAST(NULL AS INT64) id\n",
    "  from {data_set}.tmp_different_characters, UNNEST(JSON_QUERY_ARRAY(TRIM(pairs, '`json\\n'), '$')) pair;\t\t\n",
    "  UPDATE {data_set}.tmp_different_characters_split SET id = CAST(JSON_VALUE(pair, '$.id') AS INT64) where id is null;\n",
    "\n",
    "  -- remove false positives, detected by second check\n",
    "  DELETE FROM {data_set}.tmp_the_same_characters_split WHERE ID IN (SELECT id FROM {data_set}.tmp_different_characters_split);\t\t\n",
    "  -- let's leave only real duplicates in vector search resulting table\n",
    "  DELETE FROM {data_set}.tmp_characters_id_distance where query_id NOT IN (SELECT id from {data_set}.tmp_the_same_characters_split);\n",
    "\n",
    "  -- prepare for recursive query by creating table with duplicate pairs in both direction\n",
    "  CREATE OR REPLACE TABLE {data_set}.tmp_bidirectional_edges as (\n",
    "  SELECT query_id, id FROM {data_set}.tmp_characters_id_distance UNION DISTINCT SELECT id, query_id FROM {data_set}.tmp_characters_id_distance);\n",
    "\n",
    "  -- run recursive query to find whole graph of connected duplicates (A may be duplicate of B, then B may be duplicate of C and so on)\n",
    "  CREATE OR REPLACE TABLE {data_set}.tmp_grouped_duplicates_graph as\n",
    "  WITH RECURSIVE connected_components AS (\n",
    "      SELECT query_id AS node, query_id AS root, 0 AS iteration FROM {data_set}.tmp_bidirectional_edges\n",
    "      UNION ALL\n",
    "      SELECT edges.id AS node, LEAST(comp.root, edges.id) AS root, comp.iteration + 1\n",
    "      FROM connected_components comp INNER JOIN {data_set}.tmp_bidirectional_edges edges ON comp.node = edges.query_id\n",
    "      WHERE comp.iteration < 10\n",
    "  ),\n",
    "  unique_graph AS (SELECT node, MIN(root) AS group_id FROM connected_components GROUP BY node)\n",
    "  SELECT group_id, ARRAY_AGG(DISTINCT node ORDER BY node) AS duplicate_ids_array FROM unique_graph GROUP BY group_id;\n",
    "\n",
    "  -- prepare json arrays with all duplicated characters: one array per one \"real\" person\n",
    "  CREATE OR REPLACE TABLE {data_set}.tmp_grouped_duplicates_graph_enriched as\n",
    "  WITH duplicates AS (select group_id, id from {data_set}.tmp_grouped_duplicates_graph, UNNEST(duplicate_ids_array) id)\n",
    "  SELECT d.group_id, \"[\\n\" || STRING_AGG(TO_JSON_STRING(STRUCT(i.full_name, i.information), true), \",\\n\" ORDER BY i.chunk_number) || \"\\n]\" duplicated_ids\n",
    "  FROM {data_set}.identifiers i join duplicates d on i.id = d.id group by group_id;\n",
    "\n",
    "  -- another, final AI check, if they are really all duplicates\n",
    "  CREATE OR REPLACE TABLE {data_set}.tmp_merged_duplicates_double_check as SELECT x.group_id,\n",
    "  AI.GENERATE_BOOL(FORMAT(p.prompt, x.duplicated_ids, b.summary), connection_id => '{connection_id}', endpoint => '{endpoint}').result check\n",
    "  FROM {data_set}.prompts p, {data_set}.tmp_grouped_duplicates_graph_enriched x, {data_set}.identifiers i, {data_set}.books b WHERE p.code = 'merge_character_ids_double_check' and i.id = x.group_id and i.book_id = b.book_id;\n",
    "\n",
    "  -- get rid of ones which failed the above check\n",
    "  DELETE FROM {data_set}.tmp_grouped_duplicates_graph WHERE group_id IN (\n",
    "    select group_id from {data_set}.tmp_merged_duplicates_double_check where check = FALSE);\n",
    "  DELETE FROM {data_set}.tmp_grouped_duplicates_graph_enriched WHERE group_id IN (\n",
    "    select group_id from {data_set}.tmp_merged_duplicates_double_check where check = FALSE);\n",
    "\n",
    "  -- let's merge all duplicates into one final character: multiple personality disorder finally cured\n",
    "  CREATE OR REPLACE TABLE {data_set}.tmp_merged_duplicates as SELECT x.group_id,\n",
    "  AI.GENERATE(FORMAT(p.prompt, x.duplicated_ids, b.summary), connection_id => '{connection_id}', endpoint => '{endpoint}').result character_id\n",
    "  FROM {data_set}.prompts p, {data_set}.tmp_grouped_duplicates_graph_enriched x, {data_set}.identifiers i, {data_set}.books b WHERE p.code = 'merge_character_ids' and i.id = x.group_id and i.book_id = b.book_id;\n",
    "\n",
    "  -- extract some data from json result to columns\n",
    "  CREATE OR REPLACE TABLE {data_set}.tmp_merged_duplicates_split AS\n",
    "  select group_id, TRIM(character_id, '`json\\n') character_id, CAST(NULL AS STRING) full_name, CAST(NULL AS STRING) information\n",
    "  from {data_set}.tmp_merged_duplicates;\n",
    "  UPDATE {data_set}.tmp_merged_duplicates_split SET full_name = json_value(character_id, '$.full_name'), information = json_value(character_id, '$.information') where full_name is null;\n",
    "\n",
    "  -- update the chosen-to-remain duplicate (it's the one with lowest id) data with merged data\n",
    "  UPDATE {data_set}.identifiers chid SET chid.full_name = duplicate.full_name, chid.information = duplicate.information,\n",
    "  chid.characters_id_data = TO_JSON_STRING(STRUCT(duplicate.full_name, duplicate.information), true)\n",
    "  FROM {data_set}.tmp_merged_duplicates_split duplicate WHERE chid.id = duplicate.group_id;\n",
    "\n",
    "  -- split the duplicates graph array to multiple rows\n",
    "  CREATE OR REPLACE TABLE {data_set}.tmp_grouped_ids_split as select group_id, id from {data_set}.tmp_grouped_duplicates_graph, UNNEST(duplicate_ids_array) id;\n",
    "  -- count new 'importance' (concatenate all from duplicate and 'fragment_number' (concatenate all from duplicate)\n",
    "  UPDATE {data_set}.identifiers chid SET chid.importance = dp.importance, chid.fragment_number = dp.fragment_number\n",
    "  FROM (select chid.id, sum(di.importance) importance, string_agg(distinct di.fragment_number, ',') fragment_number\n",
    "  FROM {data_set}.identifiers chid JOIN {data_set}.tmp_grouped_ids_split d ON d.group_id = chid.id\n",
    "  JOIN {data_set}.identifiers di ON d.id = di.id group by chid.id) dp\n",
    "  WHERE chid.id = dp.id;\n",
    "\n",
    "  -- exterminate all duplicates but the chosen one\n",
    "  -- TODO - preserve all the information fields and later summarize them to summary much larger then current one\n",
    "  DELETE FROM {data_set}.tmp_grouped_ids_split where group_id = id;\n",
    "  DELETE FROM {data_set}.identifiers WHERE id IN (SELECT id from {data_set}.tmp_grouped_ids_split);\n",
    "  DELETE FROM {data_set}.tmp_characters_id_embeddings where id IN (SELECT id from {data_set}.tmp_grouped_ids_split);\n",
    "\n",
    "  -- updates embedding for merged duplicates\n",
    "  CREATE OR REPLACE TABLE {data_set}.tmp_characters_id_changed_embeddings as SELECT * FROM ML.GENERATE_EMBEDDING(\n",
    "    MODEL `{data_set}.gemini-embedding-001`,\n",
    "    (SELECT chid.characters_id_data as content, chid.full_name as title, chid.id FROM {data_set}.identifiers chid\n",
    "    JOIN {data_set}.tmp_grouped_duplicates_graph duplicate ON chid.id = duplicate.group_id),\n",
    "    STRUCT(TRUE AS flatten_json_output, 'RETRIEVAL_DOCUMENT' as task_type));\n",
    "\n",
    "  UPDATE {data_set}.tmp_characters_id_embeddings t SET ml_generate_embedding_result = tnew.ml_generate_embedding_result, ml_generate_embedding_statistics = tnew.ml_generate_embedding_statistics,\n",
    "    ml_generate_embedding_status = tnew.ml_generate_embedding_status, title = tnew.title, content = tnew.content\n",
    "  FROM {data_set}.tmp_characters_id_changed_embeddings tnew WHERE tnew.id = t.id;\n",
    "\n",
    "  -- if there were no duplicates, it will tell the algorithm that we may stop searching\n",
    "  SET updated = @@row_count;\n",
    "END;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0787b4a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ea8389b333749f68d54b4c21df0cf41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Query is running:   0%|          |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%bigquery --pyformat\n",
    "CREATE OR REPLACE PROCEDURE phase5_merge_characters_duplicates()  \n",
    "BEGIN\n",
    "  -- helper method for first few executions when we ignore output\n",
    "  DECLARE merged INT64 DEFAULT 0;\n",
    "  call {data_set}.phase5_merge_characters_duplicates_with_return_param(merged);\n",
    "END;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9dc22d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2987cd53fcf4b8fafc9723a43a3d119",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Query is running:   0%|          |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%bigquery --pyformat\n",
    "CREATE OR REPLACE PROCEDURE phase5_rebuild_indentifier_jsons()  \n",
    "BEGIN\n",
    "    -- update 'identifiers' table with finall, corect data format in json column\n",
    "    UPDATE {data_set}.identifiers SET characters_id_data = TO_JSON_STRING(STRUCT(id, full_name, information), true) WHERE full_name is not null;\n",
    "END;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a63ae7",
   "metadata": {},
   "source": [
    "### Phase 6 - Extracting information\n",
    "**This stage is finally gathering the data we want. In this project we chose as an example: gender, financial status, social class and moral values.**\n",
    "**In real life applications, any chosen traits can be chosen, by adapting prompts and param values in clustering stage.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a213fabc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "347b8f365fff4144a5e45b91d97bd7ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Query is running:   0%|          |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%bigquery --pyformat\n",
    "CREATE OR REPLACE PROCEDURE phase6_gather_characters_full_data_from_chunks()  \n",
    "BEGIN\n",
    "    -- lets gather characters into single group for each chunk (but no longer then 15 people for better inference later)\n",
    "    CREATE OR REPLACE TABLE {data_set}.tmp_chunks_character_id_data as \n",
    "    SELECT book_id, chunk_number, \"[\\n\" || STRING_AGG(characters_id_data, \",\\n\") || \"\\n]\" characters_id_data\n",
    "    FROM (\n",
    "        SELECT ecb.book_id, ecb.chunk_number, i.characters_id_data, CEIL(ROW_NUMBER() OVER (PARTITION BY ecb.book_id, ecb.chunk_number ORDER BY i.id) / 15.0) chunk_15_nr        \n",
    "        FROM {data_set}.chunks ecb, {data_set}.identifiers i  \n",
    "        WHERE i.book_id = ecb.book_id and i.fragment_number LIKE '%' || ecb.fragment_number || '%')\n",
    "    group by book_id, chunk_number, chunk_15_nr;\n",
    "\n",
    "    -- extract desired data for given characters from given fragments\n",
    "    CREATE OR REPLACE TABLE {data_set}.tmp_chunks_character_full_data as \n",
    "    select ccid.book_id, ccid.chunk_number, COALESCE(AI.GENERATE(FORMAT(p.prompt, ccid.characters_id_data, c.txt), \n",
    "        connection_id => '{connection_id}', endpoint => '{endpoint}',\n",
    "        model_params => JSON '{{\"systemInstruction\": {{\"parts\": [{{\"text\": \"You are an expert in extracting information from books.\"}}]}}}}').result, '[]') characters_full_data\n",
    "    FROM {data_set}.tmp_chunks_character_id_data ccid, {data_set}.prompts p, {data_set}.chunks c\n",
    "    WHERE p.code = 'extract_data' and c.book_id = ccid.book_id and c.chunk_number = ccid.chunk_number;\n",
    "\n",
    "    -- AI had important task above so it sometimes forgets to make result perfectly json formatted - no worry, we fix it here\n",
    "    UPDATE {data_set}.tmp_chunks_character_full_data b SET b.characters_full_data = \n",
    "    TRIM(AI.GENERATE(FORMAT(p.prompt, 'array', b.characters_full_data), connection_id => '{connection_id}', endpoint => '{endpoint}').result, '`json\\n')\n",
    "    FROM {data_set}.prompts p WHERE b.characters_full_data IS NOT NULL AND p.code = 'json';\n",
    "END;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d55a0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "982a1e6a515144559f217f8d45f067fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Query is running:   0%|          |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%bigquery --pyformat\n",
    "CREATE OR REPLACE PROCEDURE phase6_group_the_same_characters_data_for_merging()  \n",
    "BEGIN\n",
    "    -- split the book fragment gangs into separate persons\n",
    "    CREATE OR REPLACE TABLE {data_set}.tmp_split_full_data as\n",
    "    select book_id, character_full_data, chunk_number, CAST(NULL AS INT64) id, CAST(NULL AS STRING) full_name from {data_set}.tmp_chunks_character_full_data,\n",
    "    UNNEST(JSON_QUERY_ARRAY(characters_full_data, '$')) character_full_data where characters_full_data is not null;\n",
    "    -- and extract some data from json\n",
    "    UPDATE {data_set}.tmp_split_full_data SET full_name = json_value(character_full_data, '$.full_name'), id = CAST(JSON_VALUE(character_full_data, '$.id') AS INT64) WHERE full_name IS NULL;\n",
    "\n",
    "    -- group together the same person handling our precious information from different fragments\n",
    "    CREATE OR REPLACE TABLE {data_set}.tmp_grouped_full_data AS\n",
    "    SELECT id, book_id, count(id) as parts, \"[\\n\" || STRING_AGG(character_full_data, \",\\n\" ORDER BY chunk_number) || \"\\n]\" full_data_array,\n",
    "    CAST(NULL AS STRING) character_full_data,  CAST(NULL AS STRING) information\n",
    "    FROM {data_set}.tmp_split_full_data GROUP BY book_id, id;\n",
    "END;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2ea306",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4c9d98e288d41c388bde95d24f0a7b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Query is running:   0%|          |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%bigquery --pyformat\n",
    "CREATE OR REPLACE PROCEDURE phase6_merge_data()\n",
    "BEGIN\n",
    "    -- merger the information gathered in previous procedure into one conscise data\n",
    "    UPDATE {data_set}.tmp_grouped_full_data data SET data.character_full_data = \n",
    "    AI.GENERATE(FORMAT(p.prompt, data.full_data_array, cb.SUMMARY, ids.information), connection_id => '{connection_id}', endpoint => '{endpoint}').result\n",
    "    FROM {data_set}.prompts p, {data_set}.books cb, {data_set}.identifiers ids\n",
    "    WHERE data.character_full_data IS NULL AND data.full_data_array IS NOT NULL and data.parts > 1\n",
    "    AND p.code = 'merge_character' and data.book_id = cb.book_id and data.book_id = ids.book_id AND ids.id = data.id;\n",
    "    -- but there is no need if given person appeared in only one book fragment\n",
    "    UPDATE {data_set}.tmp_grouped_full_data SET character_full_data = TRIM(full_data_array, '[] \\n') \n",
    "    where character_full_data IS NULL AND full_data_array IS NOT NULL and parts = 1;\n",
    "\n",
    "    -- AI sometimes puts information into array of strings instead of single text field, check this and other errors\n",
    "    UPDATE {data_set}.tmp_grouped_full_data data SET data.character_full_data = \n",
    "    TRIM(AI.GENERATE(FORMAT(p.prompt, data.character_full_data), connection_id => '{connection_id}', endpoint => '{endpoint}').result, '`json\\n')\n",
    "    FROM {data_set}.prompts p WHERE data.character_full_data IS NOT NULL AND p.code = 'json_final_check';\n",
    "    -- put final date into 'information' table\n",
    "    UPDATE {data_set}.tmp_grouped_full_data a SET a.information = ids.information\n",
    "    FROM {data_set}.identifiers ids WHERE ids.id = a.id;\n",
    "END;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d719ebc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d2904f97500409fa4d58c223cec335c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Query is running:   0%|          |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%bigquery --pyformat\n",
    "CREATE OR REPLACE PROCEDURE phase6_copy_final_data()\n",
    "BEGIN\n",
    "  -- 'characters' table is the very table with final results that real users will be browsing\n",
    "  INSERT INTO {data_set}.characters (book_id, id, full_name, sex, social_class, wealth, values, information)\n",
    "  select\n",
    "    book_id,\n",
    "    id,\n",
    "    json_value(character_full_data, '$.full_name') full_name,\n",
    "    json_value(character_full_data, '$.sex') sex,\n",
    "    json_value(character_full_data, '$.social_class') social_class,\n",
    "    json_value(character_full_data, '$.wealth') wealth,\n",
    "    json_value(character_full_data, '$.values') values,\n",
    "    information\n",
    "  from {data_set}.tmp_grouped_full_data a where not exists (\n",
    "    select 1 from {data_set}.characters fa where a.id = fa.id AND a.book_id = fa.book_id\n",
    "  );\n",
    "\n",
    "  UPDATE {data_set}.characters fa SET fa.importance = ids.importance\n",
    "  FROM {data_set}.identifiers ids where ids.id = fa.id AND ids.book_id = fa.book_id AND fa.importance is null;\n",
    "  \n",
    "  -- we mark the books as processed, so it will not be processed again if whole notebook will be restarted\n",
    "  UPDATE {data_set}.books set processed = TRUE where book_id IN (\n",
    "    SELECT book_id from {data_set}.tmp_grouped_full_data);\n",
    "END;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098bf7f0",
   "metadata": {},
   "source": [
    "### Phase 7 - Clustering\n",
    "**It takes the raw data gathered for each interesting us trait and clusters them together into consistent groups.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd89d0d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2722e08080f44bb92538d576ff49e5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Query is running:   0%|          |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%bigquery --pyformat\n",
    "CREATE OR REPLACE PROCEDURE phase7_split_traits(trait_desc STRING)\n",
    "BEGIN\n",
    "    -- lets ask AI to split and sanitize our precious data, it is first step to allow some statistical analysis\n",
    "    CREATE OR REPLACE TABLE {data_set}.tmp_split_traits as \n",
    "    SELECT t.book_id, t.id, t.traits as original_traits,\n",
    "    COALESCE(AI.GENERATE(FORMAT(p.prompt, trait_desc, t.traits, i.information), connection_id => '{connection_id}', endpoint => '{endpoint}').result, '[]') traits\n",
    "    FROM {data_set}.prompts p, {data_set}.tmp_copied_traits t, {data_set}.characters i\n",
    "    WHERE p.code = 'split_traits' AND t.id = i.id AND t.book_id = i.book_id;\n",
    "\n",
    "    -- put split traits elements int separate rows\n",
    "    CREATE OR REPLACE TABLE {data_set}.tmp_split_traits_final AS\n",
    "    SELECT *, row_number() OVER(ORDER BY id, book_id) trait_id, CAST(NULL AS INT64) cluster_id FROM\n",
    "    (select book_id, id, trait\n",
    "    FROM {data_set}.tmp_split_traits, UNNEST(JSON_QUERY_ARRAY(TRIM(traits, '`json\\n'), '$')) as trait);\t\n",
    "\n",
    "    UPDATE {data_set}.tmp_split_traits_final set trait = TRIM(trait, '\"') where 1=1;\n",
    "END;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c61afbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "398ac0741b6842638da1627de926fa23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Query is running:   0%|          |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%bigquery --pyformat\n",
    "CREATE OR REPLACE PROCEDURE phase7_identify_clusters(clusters INT64)\n",
    "BEGIN\n",
    "    -- let's do some embedding on split traits, they are usually very short so 128 length should be good\n",
    "    -- note: there is 'CLUSTERING' task_type, but it works much worse here\n",
    "    CREATE OR REPLACE TABLE {data_set}.tmp_traits_embeddings as SELECT * FROM ML.GENERATE_EMBEDDING(\n",
    "    MODEL `{data_set}.gemini-embedding-001`,\n",
    "    (SELECT trait as content, trait_id FROM {data_set}.tmp_split_traits_final),\n",
    "    STRUCT(TRUE AS flatten_json_output, 'SEMANTIC_SIMILARITY' as task_type, 128 as OUTPUT_DIMENSIONALITY));\n",
    "    \n",
    "    -- prepare random sample of max 2000 embeddings\n",
    "    CREATE OR REPLACE TABLE {data_set}.tmp_traits_embeddings_modeling_sample as\n",
    "    WITH size as (select count(*) count from {data_set}.tmp_traits_embeddings)\n",
    "    SELECT ml_generate_embedding_result, content, trait_id FROM {data_set}.tmp_traits_embeddings traits, size\n",
    "    where rand() < (2000 / size.count);\n",
    "    -- and use it to train KMEANS model\n",
    "    CREATE OR REPLACE MODEL `{data_set}.kmeans_traits_model`\n",
    "    OPTIONS(MODEL_TYPE = 'KMEANS', NUM_CLUSTERS = clusters, KMEANS_INIT_METHOD = 'KMEANS++', DISTANCE_TYPE = 'COSINE') AS\n",
    "    SELECT ml_generate_embedding_result FROM {data_set}.tmp_traits_embeddings_modeling_sample;\n",
    "\n",
    "    -- lets find clusters for each split values (execute immediate to avoid procedure verification error on unknown model)\n",
    "    EXECUTE IMMEDIATE ('''CREATE OR REPLACE TABLE `{data_set}.tmp_clustered_traits` AS\n",
    "        SELECT * FROM ML.PREDICT(MODEL `{data_set}.kmeans_traits_model`,\n",
    "        (SELECT ml_generate_embedding_result, content, trait_id FROM `{data_set}.tmp_traits_embeddings`))''');\n",
    "\n",
    "    UPDATE {data_set}.tmp_split_traits_final v set v.cluster_id = cv.centroid_id\n",
    "    FROM {data_set}.tmp_clustered_traits cv WHERE v.trait_id = cv.trait_id;\n",
    "END;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692c9f17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d618f09a09a94f1e8651e1258c890c14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Query is running:   0%|          |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%bigquery --pyformat\n",
    "CREATE OR REPLACE PROCEDURE phase7_name_clusters(trait_type STRING, trait_desc STRING)\n",
    "BEGIN\n",
    "    -- for each claster gather 200 random representatives\n",
    "    CREATE OR REPLACE TABLE {data_set}.tmp_trait_clusters as\n",
    "    select centroid_id as cluster_id, ARRAY_TO_STRING(ARRAY_AGG(content LIMIT 200), ', ') examples from {data_set}.tmp_clustered_traits group by centroid_id;\n",
    "\n",
    "    -- throw the representatives at the AI and ask it to give them one common name and description\n",
    "    CREATE OR REPLACE TABLE {data_set}.tmp_clusters_analysis as\n",
    "    SELECT AI.GENERATE(FORMAT(p.prompt, trait_desc, c.json_examples), connection_id => '{connection_id}', endpoint => '{endpoint}').result\n",
    "    FROM (select '[\\n' || string_agg(json_example, ',\\n') || '\\n]' json_examples\n",
    "    FROM (select cluster_id, TO_JSON_STRING(STRUCT(cluster_id, examples)) json_example from {data_set}.tmp_trait_clusters order by cluster_id)) c\n",
    "    join {data_set}.prompts p ON p.code = 'cluster_traits';\n",
    "\n",
    "    -- extract from json and store the invented name and description in 'clusters' table\n",
    "    CREATE OR REPLACE TABLE {data_set}.tmp_cluster_names as\n",
    "    select CAST(json_value(clusters, '$.cluster_id') AS INT64) cluster_id, json_value(clusters, '$.cluster_name') name, json_value(clusters, '$.cluster_description') description\n",
    "    FROM {data_set}.tmp_clusters_analysis, UNNEST(JSON_QUERY_ARRAY(TRIM(result, '`json\\n'), '$')) as clusters;\n",
    "    DELETE FROM {data_set}.clusters where cluster_type = trait_type;\n",
    "    INSERT INTO {data_set}.clusters (cluster_type, cluster_id, name, description)\n",
    "    SELECT trait_type, cluster_id, name, description FROM {data_set}.tmp_cluster_names;\n",
    "END;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d69c5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bigquery --pyformat\n",
    "CREATE OR REPLACE PROCEDURE phase7_assign_clusters(trait_type STRING)\n",
    "BEGIN\n",
    "    -- set the cluster_ids in 'character_cluster_details'\n",
    "    DELETE FROM {data_set}.character_cluster_details where book_id IN (SELECT book_id FROM {data_set}.tmp_split_traits_final);\n",
    "    INSERT INTO {data_set}.character_cluster_details(book_id, id, trait, trait_id, cluster_type, cluster_id)\n",
    "    SELECT book_id, id, trait, trait_id, trait_type, cluster_id FROM {data_set}.tmp_split_traits_final;\n",
    "\n",
    "    -- combine traits of each person together (we already have one, but this one is sanitized)\n",
    "    CREATE OR REPLACE TABLE {data_set}.tmp_combined_traits as\n",
    "    SELECT book_id, id, cluster_type, string_agg(trait, ', ') traits, CAST(NULL AS INT64) cluster_id\n",
    "    FROM {data_set}.character_cluster_details GROUP BY book_id, id, cluster_type;\n",
    "\n",
    "    -- prepare embeddings for concatenated traits\n",
    "    CREATE OR REPLACE TABLE {data_set}.tmp_combined_traits_embeddings as SELECT * FROM ML.GENERATE_EMBEDDING(\n",
    "    MODEL `{data_set}.gemini-embedding-001`,\n",
    "    (SELECT traits as content, book_id, id FROM {data_set}.tmp_combined_traits),\n",
    "    STRUCT(TRUE AS flatten_json_output, 'SEMANTIC_SIMILARITY' as task_type, 128 as OUTPUT_DIMENSIONALITY));\n",
    "    \n",
    "    -- and find the main cluster_id for given person using previously trained\n",
    "    EXECUTE IMMEDIATE ('''CREATE OR REPLACE TABLE `{data_set}.tmp_combined_traits_clustered` AS\n",
    "        SELECT * FROM ML.PREDICT(MODEL `{data_set}.kmeans_traits_model`,\n",
    "        (SELECT ml_generate_embedding_result, book_id, id FROM `{data_set}.tmp_combined_traits_embeddings`))''');\n",
    "\n",
    "    UPDATE {data_set}.tmp_combined_traits ct set ct.cluster_id = ctc.centroid_id\n",
    "    FROM {data_set}.tmp_combined_traits_clustered ctc WHERE ctc.id = ct.id AND ctc.book_id = ct.book_id;\n",
    "END;"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "book_crawler",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
