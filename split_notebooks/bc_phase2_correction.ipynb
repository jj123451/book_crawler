{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0c93132a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't find program: 'false'\n"
     ]
    }
   ],
   "source": [
    "%%script false --no-raise-error\n",
    "import json\n",
    "from google.colab import userdata\n",
    "from google.oauth2 import service_account\n",
    "from google.cloud.bigquery import magics\n",
    "\n",
    "credentials_json = userdata.get('BIGQUERY_CREDENTIALS')\n",
    "credentials = service_account.Credentials.from_service_account_info(json.loads(credentials_json))\n",
    "magics.context.credentials = credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bc730c95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The bigquery_magics extension is already loaded. To reload it, use:\n",
      "  %reload_ext bigquery_magics\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import bigquery\n",
    "from google.cloud.bigquery import magics\n",
    "%load_ext bigquery_magics\n",
    "\n",
    "data_set = \"testing_set\"\n",
    "project_name = \"emerald-entity-468916-f9\"\n",
    "\n",
    "job_config = bigquery.QueryJobConfig(default_dataset = f\"{project_name}.{data_set}\")\n",
    "client = bigquery.Client(project = project_name, default_query_job_config = job_config, credentials = globals().get('credentials', None))\n",
    "magics.context.default_query_job_config = job_config\n",
    "magics.context.project = project_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "270626d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\jurow\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\jurow\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from pydantic import BaseModel\n",
    "from typing import List\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "class Chunk(BaseModel):\n",
    "    before: str  \n",
    "    main: str  \n",
    "    after: str\n",
    "\n",
    "def chunk_text(sentences: list, sentences_lenghts: list, max_tokens) -> tuple:\n",
    "\n",
    "    chunks = []\n",
    "    lenghts = []\n",
    "\n",
    "    current_sentences = []\n",
    "    current_lenghts = []\n",
    "    current_length = 0\n",
    "    \n",
    "    for length, sentence in zip(sentences_lenghts, sentences):\n",
    "        if current_length + length > max_tokens and current_length > 0:\n",
    "            chunks.append(current_sentences)\n",
    "            lenghts.append(current_lenghts)\n",
    "            current_sentences = []\n",
    "            current_lenghts = []\n",
    "            current_length = 0\n",
    "\n",
    "        current_sentences.append(sentence)\n",
    "        current_lenghts.append(length)\n",
    "        current_length += length\n",
    "\n",
    "    if current_length > 0:\n",
    "        chunks.append(current_sentences)\n",
    "        lenghts.append(current_lenghts)\n",
    "\n",
    "    return (chunks, lenghts)\n",
    "\n",
    "def get_prefix(sentences: list, lenghts: list, max_tokens):\n",
    "    current_sentences = []\n",
    "    current_length = 0\n",
    "    \n",
    "    for length, sentence in zip(lenghts, sentences):\n",
    "        if current_length + length > max_tokens and current_length > 0:\n",
    "            break\n",
    "\n",
    "        current_sentences.append(sentence)\n",
    "        current_length += length\n",
    "\n",
    "    return current_sentences\n",
    "\n",
    "def get_prefixes(chunks: list, lengths: list, max_tokens) -> list:\n",
    "    return [\" \".join(get_prefix(chunk_sentences, chunk_lengths, max_tokens)) for chunk_sentences, chunk_lengths in zip(chunks, lengths)]\n",
    "\n",
    "def get_suffixes(chunks: list, lengths: list, max_tokens) -> list:\n",
    "    return [\" \".join(reversed(get_prefix(chunk_sentences[::-1], chunk_lengths[::-1], max_tokens))) for chunk_sentences, chunk_lengths in zip(chunks, lengths)]\n",
    "\n",
    "def add_overlaps(chunks: list, prefixes: list, suffixes: list):\n",
    "    result = []    \n",
    "    for idx, chunk in enumerate(chunks):\n",
    "        result.append(Chunk(before = \"\" if idx == 0 else suffixes[idx - 1], main = chunk, after = \"\" if idx == len(chunks) - 1 else prefixes[idx + 1]))\n",
    "    return result\n",
    "\n",
    "# divides long text to chunks with overlaps, but as opposite to common implementations, overlaps are not added to chunks, but provided separately\n",
    "def chunk_text_with_overlaps(long_text, max_chunk_tokens, max_overlap_tokens) -> List[Chunk]:\n",
    "    sentences = sent_tokenize(long_text)\n",
    "    sentences_lenghts = [len(word_tokenize(sentence)) for sentence in sentences]\n",
    "\n",
    "    chunks_and_lenghts = chunk_text(sentences, sentences_lenghts, max_chunk_tokens)\n",
    "    chunked_sentences = chunks_and_lenghts[0]\n",
    "    chunked_sentences_lenghts = chunks_and_lenghts[1]\n",
    "\n",
    "    prefixes = get_prefixes(chunked_sentences, chunked_sentences_lenghts, max_overlap_tokens)\n",
    "    suffixes = get_suffixes(chunked_sentences, chunked_sentences_lenghts, max_overlap_tokens)\n",
    "    chunks = [\" \".join(chunk_sentences) for chunk_sentences in chunked_sentences]\n",
    "\n",
    "    return add_overlaps(chunks, prefixes, suffixes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc22b11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No books to correct.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "clean_sql = \"CREATE OR REPLACE TABLE tmp_correction_chunks(book_id STRING, chunk_number INTEGER, prefix STRING, original_txt STRING, suffix STRING, corrected_txt STRING)\"\n",
    "insert_sql = f\"INSERT INTO tmp_correction_chunks(book_id, chunk_number, prefix, original_txt, suffix) VALUES(@id, @idx, @prefix, @txt, @suffix)\"\n",
    "\n",
    "job_configs = []\n",
    "\n",
    "client.query_and_wait(query = clean_sql)\n",
    "select_query_job = client.query(query = f\"select * from books WHERE corrected_txt IS NULL\")\n",
    "any_row = False\n",
    "\n",
    "for row in select_query_job.result():\n",
    "    any_row = True\n",
    "    print(f\"\\nChunking book: {row[\"title\"]}\")\n",
    "    text_to_split = row[\"original_txt\"]\n",
    "    book_id = row[\"book_id\"]\n",
    "    chunks = chunk_text_with_overlaps(text_to_split, max_chunk_tokens = 3000, max_overlap_tokens = 1000)\n",
    "    print(f\"  Number of chunks: {len(chunks)}\")\n",
    "    print(f\"  Processed: \", end = \"\")\n",
    "    for idx, chunk in enumerate(chunks):\n",
    "        print(f\"{idx} \", end = \"\")\n",
    "        job_config = bigquery.QueryJobConfig(query_parameters=[\n",
    "            bigquery.ScalarQueryParameter(\"id\", \"STRING\", book_id),\n",
    "            bigquery.ScalarQueryParameter(\"idx\", \"INTEGER\", idx),\n",
    "            bigquery.ScalarQueryParameter(\"prefix\", \"STRING\", chunk.before),\n",
    "            bigquery.ScalarQueryParameter(\"txt\", \"STRING\", chunk.main),\n",
    "            bigquery.ScalarQueryParameter(\"suffix\", \"STRING\", chunk.after)])\n",
    "        job_configs.append(job_config)\n",
    "\n",
    "if any_row:\n",
    "    print(f\"\\n\\nExecuting {len(job_configs)} INSERT jobs...\")\n",
    "    print(\"Jobs Completed: \", end = \"\")\n",
    "\n",
    "    def execute_insert_job(job_config):\n",
    "        client.query_and_wait(insert_sql, job_config = job_config)\n",
    "\n",
    "    completed_jobs = 0\n",
    "    with ThreadPoolExecutor(max_workers = 10) as executor:\n",
    "        future_to_config = {executor.submit(execute_insert_job, config): config for config in job_configs}\n",
    "        for future in as_completed(future_to_config):\n",
    "            future.result()\n",
    "            completed_jobs += 1\n",
    "            print(f\"{completed_jobs} \", end = \"\")\n",
    "\n",
    "    print(\"\\nAll INSERT jobs completed.\")\n",
    "else:\n",
    "    print(\"No books to correct.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c7113ca0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5cd1a5381cb474f871c250cbfc2e11b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Query is running:   0%|          |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%bigquery\n",
    "CALL phase2_correction_correct();"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "book_crawler",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
